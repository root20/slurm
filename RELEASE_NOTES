RELEASE NOTES FOR SLURM VERSION 19.05
16 August 2018

IMPORTANT NOTES:
If using the slurmdbd (Slurm DataBase Daemon) you must update this first.

NOTE: If using a backup DBD you must start the primary first to do any
database conversion, the backup will not start until this has happened.

The 19.05 slurmdbd will work with Slurm daemons of version 17.11 and above.
You will not need to update all clusters at the same time, but it is very
important to update slurmdbd first and having it running before updating
any other clusters making use of it.

Slurm can be upgraded from version 17.11 or 18.08 to version 19.05 without loss
of jobs or other state information. Upgrading directly from an earlier version
of Slurm will result in loss of state information.

If using SPANK plugins that use the Slurm APIs, they should be recompiled when
upgrading Slurm to a new major release.

NOTE: The slurmctld is now set to fatal if there are any problems with
      any state files.  To avoid this use the new '-i' flag.

NOTE: systemd services files are installed automatically, but not enabled.
      You will need to manually enable them on the appropriate systems:
      - Controller: systemctl enable slurmctld
      - Database: systemctl enable slurmdbd
      - Compute Nodes: systemctl enable slurmd

NOTE: Cray/ALPS support has been removed.

NOTE: Built-in BLCR support has been removed.

NOTE: The proctrack/sgi_job plugin has been removed.

NOTE: The crypto/openssl plugin has been removed.

NOTE: The launch/poe plugin has been removed.

NOTE: The switch/nrt plugin has been removed.

NOTE: slurmd and slurmctld will now fatal if two incompatible mechanisms for
      enforcing memory limits are set. This makes incompatible the use of
      task/cgroup memory limit enforcing (Constrain[RAM|Swap]Space=yes) with
      MemLimitEnforce=yes or with JobAcctGatherParams=OverMemoryKill, which
      could cause problems when a task is killed by one of them while the other
      is at the same time managing that task. The NoOverMemoryKill setting has
      been deprecated in favour of OverMemoryKill, since now the default is
      *NOT* to have any memory enforcement mechanism.

NOTE: SLURM_FAILURE, SLURM_SOCKET_ERROR, SLURM_PROTOCOL_SUCCESS, and
      SLURM_PROTOCOL_ERROR been removed please update to SLURM_SUCCESS or
      SLURM_ERROR as appropriate.

NOTE: Limit pam_slurm_adopt to run only in the sshd context by default, for
      security reasons. A new module option 'service=<name>' can be used to
      allow a different PAM applications to work. The option 'service=*' can be
      used to restore the old behavior of always performing the adopt logic
      regardless of the PAM application context.

NOTE: The --quiet flag in sbatch will now suppress the printing of
      "Submitted batch job %u" and "Submitted batch job %u on cluster %s".

NOTE: Sreport report 'job SizesByAccount' and 'job SizesByAccountAndWckey'
      have been changed and now will default to report the root account totals
      if no Accounts= are specified. If specified, the requested accounts will
      now be displayed instead of the ones in a lower level in the tree.
      Old behavior can still be invoked using the new option 'AcctAsParent'.
      Perl API functions have been adapted removing the '_top_' in the
      respective signatures. The new parameter have also been added.

HIGHLIGHTS
==========
 -- Add select/cons_tres plugin, which offers similar functionality to cons_res
    with an entirely new code base and far greater GPU scheduling flexibility.
 -- Remove select/serial plugin.

RPMBUILD CHANGES
================

CONFIGURATION FILE CHANGES (see man appropriate man page for details)
=====================================================================
 -- Add GPU scheduling options to slurm.conf, available both globally and
    per-partition: DefCpusPerGPU and DefMemPerGPU.
 -- Add PriorityWeightAdmin to control admin priority factor of a job.

COMMAND CHANGES (see man pages for details)
===========================================
 -- Add GPU scheduling options for salloc, sbatch and srun:
   --cpus-per-gpu, -G/--gpus, --gpu-bind, --gpu-freq, --gpus-per-node,
   --gpus-per-socket, --gpus-per-task and --mem-per-gpu.
 -- If GRES are associated with specific sockets, identify those sockets in the
    output of "scontrol show node" (e.g. "Gres=gpu:4(S:0)").

OTHER CHANGES
=============
